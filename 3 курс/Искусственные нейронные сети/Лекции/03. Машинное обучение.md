Поиск закономерностей и разработка алгоритма - комплекс методов и инструментов, позволяющий компьютеру строить модели с сильной экспертизой на основе входящих из реального мира данных и накопленных человеком научных знаний.
## Основные типы задач машинного обучения
- **Обучение с учителем**:
	- **Регрессия**:
		- Имеем прецеденты
		- Отвечаем неограниченным числом
		- Пример: какая капитализация?
	- **Классификация**:
		- Имеем прецеденты
		- Ответы из ограниченного множества
		- Пример: какой фрукт на картинке?
- **Обучение без учителя**:
	- **Кластеризация**:
		- Есть только объекты
		- Хотим сформировать группы
		- Сколько их? Какие они будут по сути?
	- **Рекомендательные системы**:
		- Пользователи, предметы, оценки
		- Никогда наверняка не знаем лучшую рекомендацию
## Компоненты классической ML-задачи
- Выборка: объект и признаки (object and features)
- Ответ (target)
- Функция потерь
- Функционал качества
- Метрики
#### Пример
Выборка: объекты
- То, на чем мы обучаем модели: $x_i = (d^1, d^2, ..., d^k)$
- Объекты описываются набором признаков: $X=\{x_i\}=R^{nk}$
- В случае обучения с учителем к набору признаков добавляется таргетная переменная: $Y=\{y_i\}\in R^n$
  
Признаки: $x_i=(d^1, d^2, ..., d^k)$
- Вещественные: $d^j \in R$
	- Сколько активов на счете компании? (какое-то число)
- Бинарные: $d^j\in\{0,1\}$
	- Дефолтила ли компания последние 5 лет? (1 - да, 0 - нет)
- Категориальные $d^j\in D$
	- Отрасль фирмы (Добыча, IT, красота, ...)
- Порядковые: $d^j\in D \subset Z$
	- Какой у компании кредитный рейтинг? (1 - ВВВ, 2 - А, 3 - АА, ...)
- Множественные (в т. ч. картинки, текст): $d^j\in 2^D$
	- Кто крупнейшие конкуренты? (\{Газпром, Лукойл} или \{X5 Retail\})
  
Target: $Y=\{y_i\}$
- Вещественный: $y^i \in R$
	- Какая будет цена акции через год? (какое-то число)
- Бинарный: $y^i \in \{0,1\}$
	- Вырастет ли цена акций? (1 - да, 0 - нет)
- Множественный: $y^i\in \{0,1,2,...,d\}$
	- Какая стратегия будет лучшей? (1 - hold, 2 - buy, 3 - sell)
- Задачи без явного таргета: $y^i\in Ø$
	- Какие акции добавить в имеющийся портфель? (\{Microsoft, Apple, Pfizer\})
  
| $d^1$, Сколько активов на счете компании? | $d^2$, Дефолтила ли компания последние 5 лет? | $d^3$, Отрасль фирмы | $d^4$, Какой у компании кредитный рейтинг? | $d^5$, Кто крупнейшие конкуренты? | $y$, Сколько будет стоить 1 акция? |
| ----------------------------------------- | --------------------------------------------- | -------------------- | ------------------------------------------ | --------------------------------- | ---------------------------------- |
| 500М \$                                   | нет                                           | Металлургия          | АА                                         | ИП Кузьминов Алексей              | 300 \$                             |
| 100М \$                                   | да                                            | Ремонт               | ВВВ                                        |                                   | 100 \$                             |
| 250М \$                                   | нет                                           | Телеком              | ААА                                        | Билайн, Мегафон                   | Не листингуется                    |
## Оценка качества модели
Сколько градусов будет завтра?
- 1 модель: 32 °C
- 2 модель: 18 °C
- Истина: 25 °C
  
Как замерить ошибку наших моделей?
- $(32-25)=7$
- $(18-25)=-7$
  
Преобразуем симметрично. Например:
- $(32-25)^2=49$
- $(18-25)^2=49$
  
Ошибка моделей - это некоторая **функция**
#### Функция потерь (loss function)
У нас есть модель $a(x)$, определяющая зависимость для пар объект-таргет из выборки $X=\{(x_i,y_i)\}$. Как понять насколько она хороша? В начале оцениваем ошибку на 1 объекте, то есть зададим loss function. Одна из самых популярных - квадратичное отклонение. $L(a(x_i),y_i) = (a(x_i)-y_i)^2$
Пусть имеем набор признаков: \{Конкуренты, Капитал, Цена сейчас\}. Допустим, имеем 2 объекта:
- $x_1=(21,165,58)$, $y_1=45$
- $x_2=(45,189,101)$, $y_1=36$
  
Существует модель с оценками $a(x_1)=46$, $a(x_2)=33$. Тогда потери на каждом объекте:
- $L(x_1,y_1)=(46-45)^2=1$
- $L(x_2,y_2)=(33-36)^2=9$
#### Функционал качества и метрика
Если усреднить функцию потерь по всем объектам, то получится некоторая средняя потеря работы нашей модели $a(x)$ на выборке $X$ из $m$ объектов.  
$$Q(a(x), X)=\frac{1}{m}\displaystyle\sum^m_{i=1}L(a(x_i),y_i)$$
**Метрика** - критерий, по которому мы окончательно замеряем качество модели. Обычно совпадает с функционалом качества.
На предыдущем примере, в котором:
- $y_1=45$, $a(x_1)=46$
- $y_1=36$, $a(x_1)=33$
  
можно замерить функционал качества: $Q=\frac{1}{2}(9+1)=5$  
В разных задачах - разные метрики.  
$IoU=\frac{Area\ of\ Overlap}{Area\ of\ Union}$
## MSE и MAE
$MSE=\frac{1}{n}\displaystyle\sum^n(a(x_i)-y_i)^2$  
$MAE=\frac{1}{n}\displaystyle\sum^n|a(x_i)-y_i|$  
![MSE и MAE](../Pictures/03_01.%20MSE%20и%20MAE.png)
#### Пример
Пусть есть 2 объекта:
- $y_1=5$
- $y_2=10$
  
И 2 модели соответственно:
- $a(x_1)=5$, $b(x_1)=6$
- $a(x_2)=512$, $b(x_2)=11$
  
Ошибки на каждом объекте:
- $a(x_1-y_1)=0$, $b(x_1)-y_1=1$
- $a(x_2-y_2)=2$, $b(x_2)-y_2=1$
  
$MAE_a=\frac{1}{2}(0+2)=1$  
$MAE_b=\frac{1}{2}(1+1)=1$  
$MSE_a=\frac{1}{2}(0^2+2^2)=2$  
$MSE_b=\frac{1}{2}(1^2+1^2)=1$  
## Оптимизационная задача
#### Оптимизация функционала качества
$$Q(a(x),X)=\frac{1}{m}\displaystyle\sum^m_{i=1}L(a(x_i),y_i)\rightarrow min$$
Процесс построения модели можно свести к некоторой оптимизационной задаче. Это либо минимизация, либо максимизация функционала - в зависимости от того, какими именно метриками мы хотим оперировать, и что демонстрирует функция ошибки (качества)
## Градиентный спуск
#### Функция одной переменной
- Инициализируемся в случайной точке $X_{start}$
- До сходимости:
	- $step=f'(X_{start})$
	- $X_{next}=X_{start}-\eta_i⋅step$
	- $X_{start}=X_{next}$
- 3 варианта порога (threshold):
	- $|f'(X_{start})|\leq \xi$
	- $|f(X_{next}-f(X_{start}))|\leq \xi$
	- $|X_{start}-X_{next}|\leq \xi$
  
![Градиентный спуск: функция 1 переменной](../Pictures/03_02.%20Градиентный%20спуск.%20Функция%201%20переменной.png)  
#### Функция двух переменных
- Инициализируемся в случайной точке $X_{start}$
- До сходимости:
	- $step=\nabla z'(X_{start})$
	- $X_{next}=X_{start}-\eta_i⋅step$
	- $X_{start}=X_{next}$
- 3 варианта порога (threshold):
	- $||\nabla z'(X_{start})||\leq \xi$
	- $|f(X_{next}-f(X_{start}))|\leq \xi$
	- $|X_{start}-X_{next}|\leq \xi$
  
![Градиентный спуск: функция 2 переменных](../Pictures/03_03.%20Градиентный%20спуск.%20Функция%202%20переменных.png)  
#### Проблема взрыва градиента
![Проблема взрыва градиента](../Pictures/03_04.%20Проблема%20взрыва%20градиента.png)
#### Проблема затухания градиента
![Проблема взрыва градиента](../Pictures/03_05.%20Проблема%20затухания%20градиента.png)
#### Проблема локального оптимума
![Проблема локального оптимума](../Pictures/03_06.%20Проблема%20локального%20оптимума.png)  
#### Проблема инициализации
![Проблема инициализации](../Pictures/03_07.%20Проблема%20инициализации.png)  
#### Итоги по градиентному спуску
- Универсальный метод
- Можно вычислять параллельно
- Легко переделать для задачи поиска максимума функции
- Настройка инициализации, длины шага и критерия останова сводится к экспериментам и все равно занимает много времени
#### Стохастический градиентный спуск
Базовая идея - считать градиент по подвыборке объектов.
- $step=\nabla z'(X_{start})$
- $X_{next}=X_{start}-\eta_i⋅step$
- $\eta_{i+1}<\eta_i$
  
Например, $\eta_i=\frac{1}{i}$  
Делаем больше шагов, но из-за экономии в расчетах выигрываем по времени. Важно делать длину шага убывающей, чтобы не отдаляться рандомно от экстремума.
#### Adagrad
- $step=\nabla z'(X_{start})$
- $(X_{next})_j=(X_{start})_j-\frac{\eta_i}{\sqrt{G_{next,j}+\varepsilon}}(step)_j$
- $G_{next,j}=G_{start,j}+(step)^2_j$
  
Хотим решить проблему со сложным выбором шага. Для каждого признака отдельно настраивается **длина** шага. Чем больше предыдущий - тем сильнее глушим следующий, так мы избегаем постоянного перескакивания минимума.
## Линейные модели
Построим нашу первую линейную модель:
- $a(x)=\beta d_1$
- $a_1(x)=0.5d_1$
- $a_2(x)=2d_1$
- $a_3(x)=1d_1$
  
Это целое семейство моделей  
![Линейная модель](../Pictures/03_08.%20Линейная%20модель.png)  
Обычно к линейному семейству принято добавлять **свободный коэффициент**: $a(x)=\beta d_1+\beta_0$. Это равносильно добавлению константного признака в объекты выборки:
- $x_1(d_1^1,d_0)=(3,1)$, $y_1=6$
- $x_2(d_1^2,d_0)=(9,1)$, $y_1=8$
- ...
- $x_1(d_1^n,d_0)=(13,1)$, $y_1=9$
  
![Линейная модель со свободным коэффициентом](../Pictures/03_09.%20Линейная%20модель%20со%20свободным%20коэффициентом.png)  
Обычно задача $M0$ - найти такие параметры в выбранном семействе, при которых зависимость между $Y$ и $X$ оценена наиболее правдоподобно. При правильно выбранном Loss'e, это равносильно поиску минимума функционала качества.  
$\beta^∗=argmin\ Q(a(x),X)$  
![Линейная модель](../Pictures/03_10.%20Линейная%20модель.png)
## Многомерная линейная регрессия
Теперь строим не прямую, а плоскость:
- $a(x)=\beta_1 d_1+\beta_2 d_2$
- $a_1(x)=0d_1+0.2d_2$
- $a_2(x)=5d_1+4d_2$
  
Но еще лучше, если добавим константу:
- $a(x)=\beta_1d_1+\beta_2d_2+\beta_0$
- $a_3(x)=2d_1+3d_2+5$
- $\beta^∗_1=2$, $\beta^∗_2=3$, $\beta^∗_0=5$
  
![Многомерная линейная регрессия](../Pictures/03_11.%20Многомерная%20линейная%20регрессия.png)  
Лучшие параметры модели - те, которые минимизируют функционал ошибки  
$\beta^∗=argmin\ Q(a(x),X)$  
![Многомерная линейная регрессия](../Pictures/03_12.%20Многомерная%20линейная%20регрессия.png)  
#### Линейная регрессия OLS
Найти лучшие $\beta$ - значит найти такие параметры, при которых $Q(a(x),X)$ минимальна.  
Пытаемся предсказать цену акции компаний в \$ через год по текущей цене $(d^1)$ и сумме кредиторской задолженности в миллионах \$ ($d^2$). Допустим, в выборку попало всего 3 компании $(x_1, x_2, x_3)$ с соответствующими ответами $(y_1, y_2, y_3)$.  
![Линейная регрессия OLS](../Pictures/03_13.%20Линейная%20регрессия%20OLS.png)  
Тогда мы можем записать среднеквадратичное отклонение, т. е. наш функционал: $Q(a(x),X)=\frac{1}{m}\displaystyle\sum^m(a(x_i)-y_i)^2=\frac{1}{3}[(a(x_1)-y_1)^2+(a(x_2)-y_2)^2+(a(x_3)-y_3)^2]=\frac{1}{3}[(\beta_1\cdot23+\beta_2\cdot0.5+\beta_0-55)^2+(\beta_1\cdot35+\beta_2\cdot1+\beta_0-100)^2+(\beta_1\cdot18+\beta_0-45)^2]\rightarrow min$
```math
\begin{cases}
    Q'_{b_1}=\frac{1}{3}[23\cdot2(beta_1\cdot23+\beta_2\cdot0.5+\beta_0-55)+35\cdot2(\beta_1\cdot35+\beta_2\cdot1+\beta_0-100)+18\cdot2(\beta_1\cdot18+\beta_0-45)]=0 \\
    Q'_{b_2}=\frac{1}{3}[0.5\cdot2(beta_1\cdot23+\beta_2\cdot0.5+\beta_0-55)+1\cdot2(\beta_1\cdot35+\beta_2\cdot1+\beta_0-100)]=0 \\
    Q'_{b_3}=\frac{1}{3}[1\cdot2(beta_1\cdot23+\beta_2\cdot0.5+\beta_0-55)+1\cdot2(\beta_1\cdot35+\beta_2\cdot1+\beta_0-100)+1\cdot2(\beta_1\cdot18+\beta_0-45)]=0
\end{cases}
```
$(\beta^∗_1,\beta^∗_2,\beta^∗_0)=(5,-30,-45)$  
Выпуклость очевидна из формы функционала. Но можно проверить еще и матрицу Гессе
#### Матричная форма
Задача поиска оптимальных коэффициентов сводится к минимизации произведения 2 матриц. Можно взять матричный дифференциал и приравнять его к нулю. Этот шаг аналогичен нахождению частных производных и решению системы для поиска критических точек.  
$Q=\frac{1}{n}(XB-Y)^T(XB-Y)\rightarrow min$  
$dQ(\beta^∗)=0$  
Взятие матричного дифференциала рассматривать не будем, запишем сразу результат:  
```math
\beta^∗=(X^TX)^{-1}X^TY=
\begin{matrix}
	+5 \\
	-30 \\
	-45
	\end{matrix} 
```
Линейная регрессия, как все линейные модели, хорошо справляется с простыми задачами на небольшом объеме данных. Например, линейная регрессия легко победит проблему прогнозирования при условии, что в данных представлены несложные (например, линейные) взаимозависимости. Также линейные подходы просты к интерпретации и пригождаются, когда интересны данные сами по себе и зависимости между переменными.