**Рекуррентные нейронные сети** - Recurrent neural network (RNN)
## Отличие RNN от CNN
- CNN ориентированы на анализ изображений, являющихся моментальными снимками состояний группы объектов в некоторый момент времени (нет анализа предыстории)
- RNN ориентированы на анализ временной последовательности состояний группы объектов
## Типы приложений
1. `1:1` - один вход, один выход без запоминания состояния (распознавание изображений)  
	![1:1](../Pictures/05_01.%201-1.png)
2. `1:M` - последовательность на выходе (вход - картинка, выход - ее словесное описание)  
	![1:M](../Pictures/05_02.%201-M.png)
3. `M:1` - последовательность на входе (вход - рецензия на фильм, выход - оценка по 10-балльной шкале)  
	![M:1](../Pictures/05_03.%20M-1.png)
4. `1×M : 1×M` - последовательность на входе, последовательность на выходе (перевод текста с одного языка на другой)  
	![1×M : 1×M](../Pictures/05_04.%201xM-1xM.png)
5. `M:M` - синхронизированные последовательности на входе и выходе (разметка смены кадра на видео, очистка аудиопотока от посторонних шумов)  
	![M:M](../Pictures/05_05.%20M-M.png)
  
Условные обозначения:  
![Условные обозначения](../Pictures/05_06.%20Условные%20обозначения.png)
  
## Область применения рекуррентных нейронных сетей
- Анализ временных рядов
	- Изменения цен акций
	- Показания датчиков
- Понимание (письменных) текстов на естественном языке
- Машинный перевод
- Понимание человеческой речи (перевод речи в текст)
- Обнаружение аномалий (определение предынфарктного состояния по кардиограмме)
- Классификация и аннотация текстов
- Предсказательная аналитика
  
![Применение RNN](../Pictures/05_07.%20Применение%20RNN.png)
## Идея RNN
Запоминать что-то из истории приходящих на вход данных, сохраняя некое внутреннее состояние, которое можно было бы потом использовать для анализа текущих данных.
## Простейшая рекуррентная сеть
![Простейшая рекуррентная сеть](../Pictures/05_08.%20Простейшая%20рекуррентная%20сеть.png)  
#### Работа рекуррентной сети
![Работа рекуррентной нейронной сети](../Pictures/05_09.%20Работа%20рекуррентной%20нейронной%20сети.png)  
![Работа рекуррентной нейронной сети](../Pictures/05_10.%20Работа%20рекуррентной%20нейронной%20сети.png)
#### Развертка рекуррентной сети
![Развертка рекуррентной сети](../Pictures/05_11.%20Развертка%20рекуррентной%20сети.png)
#### Работа развертки ($\tau=4$)
![Работа развертки](../Pictures/05_12.%20Работа%20развертки.png)  
![Работа развертки](../Pictures/05_13.%20Работа%20развертки.png)  
![Работа развертки](../Pictures/05_14.%20Работа%20развертки.png)  
#### Развертка => сеть прямого распространения
![Развертка - сеть прямого распространения](../Pictures/05_15.%20Развертка%20-%20сеть%20прямого%20распространения.png)  
![Развертка - сеть прямого распространения](../Pictures/05_16.%20Развертка%20-%20сеть%20прямого%20распространения.png)  
![Развертка - сеть прямого распространения](../Pictures/05_17.%20Развертка%20-%20сеть%20прямого%20распространения.png)
## Модель Simple RNN
1) $s^{(t)}=\tanh(Ws^{(t-1)}+Ux^{(t)}+b)$
2) $z^{(t)}=Vs^{(t)}+d$
3) $a^{(t)}=softmax(z^{(t)})$
  
$t=1,2,3,...$  
$s^{(t)}$ - вектор начального состояния  
$b$, $d$ - векторы смещений  
![Модель Simple RNN](../Pictures/05_18.%20Модель%20Simple%20RNN.png)
#### Развертка Simple RNN
![Развертка Simple RNN](../Pictures/05_19.%20Развертка%20Simple%20RNN.png)  
#### Суммарная ошибка
$C_{(\{x^{(1)},...,x^{(\tau)}\},\{y^{(1)},...,y^{(\tau)}\})}=\displaystyle\sum^\pi_{t=1}C_{(x^{(t)},y^{(t)})}$
## Глубокие RNN
1. Вставить глубокую сеть перед слоем внутреннего состояния (позволяет понимать временную структуру информации, например, выделять слова при распознавании речи)
2. Вставить глубокую сеть после слоя внутреннего состояния (позволяет выявлять закономерности, не зависящие от времени)
3. Использовать глубокую сеть вместо $\tanh$ (фрагментация изображений)
4. Использовать выходы одной RNN в качестве входов другой RNN (каждый слой действует в своем масштабе времени)
## Двунаправленные RNN
Позволяют получить 2 состояния, отражающие контекст как слева, так и справа для каждого элемента последовательности.  
Пример: разметка слов в предложении по частям речи.  
![Двунаправленные RNN](../Pictures/05_20.%20Двунаправленные%20RNN.png)
## Недостатки RNN
- Не позволяют надолго запоминать информацию во временной последовательности: влияние текущего состояния на будущие состояния экспоненциально затухает
- Градиенты, распространяющиеся через много одинаковых слоев, либо исчезают (в большинстве случаев), либо начинают взрывообразно расти (редко, но с большим уроном для обучения)
- Высокая вычислительная сложность: не удается обучать RNN с окном наблюдения размером 10 и более
## Как преодолеть забывчивость RNN?
#### Вентильные RNN (Gated RNN)/
![Вентильные RNN](../Pictures/05_21.%20Вентильные%20RNN.png)
#### Модель долгой краткосрочной памяти (long short-term memory - LSTM)
- Вентиль забывания: $f^{(t)}=\sigma(W_fz^{(t-1)}+U_fx^{(t)}+b_f)$
- Вентиль входа: $g^{(t)}=\sigma(W_gz^{(t-1)}+U_gx^{(t)}+b_g)$
- Вентиль выхода: $q^{(t)}=\sigma(W_qz^{(t-1)}+U_qx^{(t)}+b_q)$
- Состояние: $s^{(t)}=f^{(t)}\cdot s^{(t-1)}+g^{(t)}\cdot \sigma(W_sz^{(t-1)}+U_sx^{(t)}+b_s)$
- Выход: $z^{(t)}=q^{(t)}\cdot\tanh(s^{(t)})$
  
![Модель долгой краткосрочной памяти](../Pictures/05_22.%20Модель%20долгой%20краткосрочной%20памяти.png)  
Применения LSTM:
- Большинство применений RNN используют модель LSTM
- Ключевыми компонентами LSTM являются вентиль забывания и функция активации $\tanh$ перед вентилем выхода (без вентиля забывания качество сразу сильно падает, а без функции активации на выходе этот самый выход теоретически может расти неограниченно, что приводит к нежелательным эффектам)
- Основной недостаток LSTM - большое количество матриц весов (8 против 3 у Simple RNN)
  
Основная идея LSTM:  
![Основная идея LSTM](../Pictures/05_23.%20Основная%20идея%20LSTM.png)  
![Основная идея LSTM](../Pictures/05_24.%20Основная%20идея%20LSTM.png)  
Разбор LSTM:  
- $f_t=\sigma(W_f\cdot[h_{t-1},x_t]+b_f)$  
	![Разбор LSTM](../Pictures/05_25.%20Разбор%20LSTM.png)
- $i_t=\sigma(W_i\cdot[h_{t_1},x_t]+b_i)$  
	$\tilde{C}_t=\tanh(W_C\cdot[h_{t-1},x_t]+b_C)$  
	![Разбор LSTM](../Pictures/05_26.%20Разбор%20LSTM.png)
- $C_t=f_t\cdot C_{t-1}+i_t\cdot\tilde{C}_t$  
	![Разбор LSTM](../Pictures/05_27.%20Разбор%20LSTM.png)
- $o_t=\sigma(W_o[h_{t-1},x_t]+b_o)$  
	$h_t=o_t\cdot\tanh(C_t)$  
	![Разбор LSTM](../Pictures/05_28.%20Разбор%20LSTM.png)