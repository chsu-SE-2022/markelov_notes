## Виды связей в статистике
Важной задачей статистики является изучение статистических закономерностей, знание которых дает основу для предсказания и управления социально экономическими процессами. Прежде чем приступить к изучению связи между явлениями необходимо выяснить вид связи между факторным и результативным признаками.  
Различают два вида связи:
1. **Функциональная связь** - когда каждому значению величины факторного признака соответствует только одно значение результативного. При этом зависимость проявляется с одинаковой силой у всех единиц совокупности. Ее можно представить уравнением $y=f(x)$, где $у$ - результативный признак, $x$ - факторный, $f(x)$ – известная функция
2. **Статистическая связь (стохастическая зависимость)** - если причинная зависимость проявляется не в каждом отдельном случае, а в общем, среднем при большом числе наблюдений, то такая зависимость называется стохастической. Частным случаем стохастической связи является корреляционная связь, при которой изменение среднего значения результативного признака обусловлено изменением факторных признаков. Ее можно представить в виде: $y=f(x)+ε$ где $f(x)$ – известная функция, а $ε$ – часть результативного признака, определяемая неучтенными и неконтролируемыми признаками.
  
**По направлению связи** выделяют: 
- прямую связь (направление изменения результативного признака совпадает с направлением признака-фактора)
- обратную связь
  
**По аналитическому выражению (форме)** выделяют: 
- прямолинейные (или просто линейные) связи
- нелинейные (криволинейные) связи: параболы, гиперболы: степенная, показательная, экспонентная и т.д.
  
**Для корреляционных связей различают:** 
- **Прямую корреляцию** - связь между одним признаком-фактором и результативным признаком (при абстрагировании влияния других факторов)
- **Множественную корреляцию** - связь между несколькими факторными признаками и результативным (многофакторная связь)
  
**По степени тесноты связи** различают количественные критерии оценки тесноты связи:

- до $±0.3$ - практически отсутствует
- $±0.3$ до $±0.5$ - слабая
- $±0.5$ до $±0.7$ - умеренная
- $±0.7$ до $±1.0$ - сильная
  
Взаимосвязи между общественными явлениями, установленные на основе теоретического анализа, могут быть изучены, измерены и количественно выражены с помощью различных статистических методов.  
Для исследования функциональных связей применяется **балансовый метод** - характеризует зависимость между источниками формирования ресурсов и их использованием $Он + П = В + Ок$, где: $Он$, $Ок$ – остатки на начало и конец; $П$, $В$ – поступление и выбытие и индексный метод.  
Для изучения корреляционных связей используются:
1. для связей между атрибутивными признаками:
	- метод взаимной сопряженности
	- метод ассоциации
	- метод контингенции
2. для количественно варьирующих признаков:
	- метод параллельных рядов
	- метод ранговой корреляции
	- графический метод 
	- метод аналитических группировок
	- корреляционно-регрессионный анализ
## Параметрические методы изучения связи
При статистических исследованиях корреляционных связей одной из главных задач является определение формы корреляционной связи, т.е. построение модели связи. Под формой связи понимают тенденцию, которая проявляется в изменении результативного признака в связи с изменением признака-фактора. Построение и анализ корреляционной модели связи осуществляются с помощью корреляционно-регрессионного анализа, который заключается в построении и анализе статистической модели в виде уравнения регрессии (уравнения корреляционной связи), приближенно выражающей зависимость результативного признака от одного или нескольких признаков-факторов.  
Он состоит из следующих этапов:
1. Предварительного априорного анализа;
2. Сбора информации и ее первичной обработки;
3. Построения модели (уравнения регрессии);
4. Оценки и анализа модели.
  
Все этапы взаимосвязаны между собой, границы их часто переплетаются и носят условный характер.  
Выбор формы связи решается на основе теоретического анализа существа изучаемых явлений и исследования эмпирических данных. Эмпирическое исследование формы связи включает построение графиков корреляционных полей, эмпирических линий регрессии, а также анализ параллельных рядов.  
Рассмотрим однофакторную регрессию.  
Зависимости могут быть линейными и нелинейными.  
Если связь между признаками $у$ и $х$ криволинейная и описывается уравнением параболы второго порядка: $\overline{Y_x}=a_0=a_1x+a_2x^2$ то система нормальных уравнений имеет вид:
```math
\documentclass{article}
\usepackage{amsmath}
\begin{document}
\[
\begin{cases}
    na_0+a_1\sum{x}+a_2\sum{x^2}=\sum{y}, \\
    a_0\sum{x}+a_1\sum{x^2}+a_2\sum{x^3}=\sum{yx}, \\
    a_0\sum{x^2}+a_1\sum{x^3}+a_2\sum{x^4}=\sum{yx^2}
\end{cases}
\]
\end{document}
```
Если уравнение гиперболы вида: $\overline{Y_x}=a_0+\frac{a_1}{x}$, система нормальных уравнений:
```math
\documentclass{article}
\usepackage{amsmath}
\begin{document}
\[
\begin{cases}
    na_0+a_1\sum{\frac{1}{x}}=\sum{y}, \\
    a_0\sum{\frac{1}{x}}+a_1\sum{\frac{1}{x^2}}=\sum{\frac{y}{x}}
\end{cases}
\]
\end{document}
```
Наиболее часто для определения формы корреляционной связи используют уравнение прямой $y_x=a_0+a_1x$, где $х$ результативного признака $у$ от факторного показателя $х$
## Линейная форма связи и оценка ее параметров
Уравнение связи называется **уравнением регрессии**, а анализ, производимый с его помощью, называется **регрессионным**.  
После установления вида функции для модели связи определяются параметры уравнения $(a_0,a_1)$. В уравнениях регрессии параметр $a_0$ показывает усредненное влияние на результативный признак неучтенных (не выделенных для исследования) факторов; параметр $a_1$ (а в уравнении параболы еще и $a_2$) – это **коэффициент регрессии**, который показывает, насколько изменяется в среднем значение результативного признака при изменении факторного на единицу его собственного изменения.  
Оценивание неизвестных параметров производится методом наименьших квадратов (МНК), который дает систему нормальных уравнений:
```math
\documentclass{article}
\usepackage{amsmath}
\begin{document}
\[
\begin{cases}
    na_0+a_1\sum{x}=\sum{y}, \\
    a_0\sum{x}+a_1\sum{x^2}=\sum{xy}
\end{cases}
\]
\end{document}
```
решая которые находятся неизвестные параметры:  
$a_0=\frac{\sum{y}\sum{x^2}−\sum{xy}\sum{x}}{n\sum{x^2}−\sum{x}\sum{x}}$  
$a_1=\frac{n\sum{xy}-\sum{x}\sum{y}}{n\sum{x^2}-\sum{x}\sum{x}}$
## Проверка адекватности регрессионной модели
Среднеквадратичное отклонение результативного признака $y_i$ от выровненных $y_x=\sigma_{ост}$ - **остаточная дисперсия**: $\sigma_{ост}=\sqrt{\frac{\sum{(y_x-y_i)^2}}{n}}$  
Среднеквадратичное отклонение факторного признака $x_i$ от средней $\overline{x}$: $\sigma\sqrt{\frac{\sum{(x_i-\overline{x})^2}}{n}}$  
**Общая дисперсия**: $\sigma{^2_y}=\frac{\sum{(y_i-\overline{y})^2}}{n}$  
Среднеквадратичное отклонение модельных значений от средней: $\sigma_{y_x}=\sqrt{\frac{\sum(y_x-\overline{x})}{n}}$  
**Межгрупповая дисперсия** измеряет вариацию групповых средних $x_i$ вокруг общего среднего. Она измеряет вариацию, обусловленную признаком, положенным в основу группировки: $δ^2=\frac{\sum{(\overline{x_i}-\overline{x})^2f_i}}{\sum{f_i}}$  
Для проверки значимости коэффициентов линейной регрессии $y_x=a_0+a_1x$ при $n<30$ используют **t-критерий Стьюдента**. Для этого вычисляют расчетные значения t-критерия для параметра:  
$a_0:\ t_{a_0}=|a_0|\frac{\sqrt{n-2}}{\sigma_{ост}}$  
$a_1:\ t_{a_1}=|a_1|\frac{\sigma_x\sqrt{n-2}}{\sigma_{ост}}$  
Полученные значения $t_{a_0}$, $t_{a_1}$ сравнивают с критическими $t_{кр}$, которые определяют по таблице Стьюдента при заданном уровне значимости $α$ и числами степеней свободы $\nu=n-m=n-2$. Параметр признается значимым, если $t_{расч}>t_{кр}$.  
Теснота корреляционной связи между $x$ и $y$ может быть измерена **эмперическим корреляционным отношением**: $η_э=\sqrt{\frac{δ^2}{σ_y^2}}$. Чем ближе оно к 1, тем теснее связь. При 0 связи нет.  
Теснота корреляционной связи между $x$ и $y$ при заданной зависимости определяется **индексом корреляции**: $R=\sqrt{\frac{\sigma^{2}_{y}}{\sigma^{2}_{y}}}=\sqrt{1-\frac{\sigma^{2}_{ост}}{\sigma^{2}_{y}}}=\sqrt{1-\frac{\sum\limits(y-\overline{y})^{2}}{\sum\limits(y-\hat{y})^{2}}}$. Чем ближе $R$ к 1, тем теснее связь. При $R=0$ связи нет.  
Величину $R^2$ называют **коэффициентом детерминации**. Коэффициент детерминации характеризует, какая часть общей вариации $у$ объясняется изучаемым фактором $х$.  
Показателем тесноты линейной связи является **линейный коэффициент корреляции**: $r=\frac{\overline{xy}−\overline{x}\overline{y}}{σ_xσ_y}=\frac{∑(x−\overline{x})(y−\overline{y})}{nσ_xσ_y}$, $-1≤r≤1$  
Величину $r^2$ называют **линейным коэффициентом детерминации**.  
Для оценки значимости коэффициента корреляции $r$ используют $t$-критерий Стьюдента. Для этого вычисляют расчетные значения t-критерия: $t_{расч}=|r|\sqrt{\frac{n−2}{1−r^2}}$, где $n−2$ - число степеней свободы при заданном уровне значимости $α$ (обычно $α=0ю05$). Расчётное значение $t_{расч}$ сравнивают с табличными значениями $t$-критерия $t_{кр}$, которые определяются по таблице Стьюдента при заданном уровне значимости $α$.
## Непараметрические методы оценки корреляции связи
Для измерения тесноты связи согласованного варьирования атрибутивных варьирующих признаков применяются разные показатели.  
Наиболее общим из них является **коэффициент взаимной сопряженности А. А. Чупрова**. Он применяется для измерения связи между варьированием двух атрибутивных признаков, когда это варьирование образует несколько (3 и более) групп.
$φ^2=\sum{\frac{\sum{f_i^2}/\sum{f_i}}{\sum{f_i}}}$  
$K_Ч=\sqrt{\frac{φ^2}{\sqrt{(m_1-1)(m_2-1)}}}$, где $m$ – число групп по каждому признаку. Он изменяется от 0 до 1, но уже при значении 0,3 можно говорить о наличии тесной связи между вариацией изучаемых признаков.  
Если вариация обоих альтернативных признаков ограниченна двумя группами, то коэффициент взаимной сопряженности может быть исчислен значительно проще – через **коэффициент ассоциации Д. Юла** и **коэффициент контингенции К. Пирсона** ($K_K$). Для этого исходные данные сводятся в комбинационную четырехклеточную таблицу (таблица четырех полей):  

| Признак (работа) | Да (муж.) | Нет (жен.) |
| ---------------- | --------- | ---------- |
| Да               | A         | B          |
| Нет              | C         | D          |
  
$K_a=\frac{ad−bc}{ad+bc}$  
$K_K=\frac{ad−bc}{\sqrt{(a+b)(b+d)(a+c)(c+d)}}$  
где $a, b, c, d$ - числа таблицы.  
Значения коэффициентов лежат в интервале $−1<K_K<1$, чем ближе к 1, -1, тем сильнее связь. Если показатель отсутствует, то его заменяют единицей для $K_K$.  
Для определения тесноты связи как между количественными, так и между качественными признаками (если их можно проранжировать или упорядочить) можно использовать. **Коэффициент Фахнера и коэффициент ранговой корреляции Спирмена:** $K_Ф=\frac{C−H}{C+H}$, где $C$ - число совпадений знаков значений признака от средних арифметических, $H$ – число несовпадений. От -1 до 1. Если 1 - имеется полностью согласованная прямая изменчивость, если 0 - изменчивость полностью несогласуется, если -1 - имеется полная обратная согласованная изменчивость.  
$P=1-\frac{6\sum{d_i^2}}{N(N^2-1)}$  
- $0.1≤P≤0.3$ - теснота связи слабая
- $0.3≤P≤0.5$ - теснота связи умеренная
- $0.5≤P≤0.7$ - теснота связи заметная
- $0.7≤P≤0.9$ - теснота связи высокая
- $0.9≤P≤1$ - теснота связи весьма высокая
