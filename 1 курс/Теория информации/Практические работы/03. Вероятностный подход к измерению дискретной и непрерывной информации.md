**Цель:** научиться вычислять вероятности событий (появление символов в сообщении) и рассчитывать энтропию.  
1. Определить количество информации (по Хартли), содержащееся в заданном сообщении, при условии, что значениями являются буквы кириллицы: 
	«Фамилия Имя Отчество» завершил ежегодный съезд эрудированных школьников, мечтающих глубоко проникнуть в тайны физических явлений и химических реакций.  
2. Построить таблицу распределения частот символов, характерныx для заданного сообщения путём деления количества определённого символа в данном сообщении на общее число символов. По формуле $H=\sum{_{i=1}^{m}}(p_i*\log{p_i})$ вычислить энтропию сообщения
3. По формуле Шеннона для определения количества информации $l=-\log{P}=-k\sum{_{i-1}^m}(p_i*\log{p_i})$ вычислить количество информации в передаваемом сообщении   
4. Вычислить избыточность $D$ по формуле $D=1-\frac{H_p}{H_o}=1-\frac{n_o}{n_p}=\frac{n_p-n_o}{n_p}$
  
**Контрольные вопросы:**  
1. Дать определение понятие энтропия
2. Что означает вероятностный способ измерения информации?
3. Что означает статическое определение вероятности?
4. Запишите уравнение Хартли
5. Какие основные разработки внес в основу теории информации Шеннон?